library(chatgpt)
my_API <- "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"
hey_chatGPT <- function(answer_my_question) {
chat_GPT_answer <- POST(
url = "https://api.openai.com/v1/chat/completions",
add_headers(Authorization = paste("Bearer", my_API)),
content_type_json(),
encode = "json",
body = list(
model = "gpt-3.5-turbo-0301",
messages = list(
list(
role = "user",
content = answer_my_question
)
)
)
)
paste(str_trim(httr::content(chat_GPT_answer)$choices[[1]]$message$content), "TOKENS USED: ", httr::content(chat_GPT_answer)$usage$total_tokens)
}
paste(hey_chatGPT(paste("Look at the provided text. Could you summarize the provided text?", cat(content(myCorpus[[1]]), sep = "\n"))))
install.packages("openai")
library(openai)
library(chatgpt)
library(openai)
my_API <- "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"
hey_chatGPT <- function(answer_my_question) {
chat_GPT_answer <- POST(
url = "https://api.openai.com/v1/chat/completions",
add_headers(Authorization = paste("Bearer", my_API)),
content_type_json(),
encode = "json",
body = list(
model = "gpt-3.5-turbo-0301",
messages = list(
list(
role = "user",
content = answer_my_question
)
)
)
)
paste(str_trim(httr::content(chat_GPT_answer)$choices[[1]]$message$content), "TOKENS USED: ", httr::content(chat_GPT_answer)$usage$total_tokens)
}
paste(hey_chatGPT(paste("Look at the provided text. Could you summarize the provided text?", cat(content(myCorpus[[1]]), sep = "\n"))))
install.packages("httr")
install.packages("httr")
library(httr)
paste(hey_chatGPT(paste("Look at the provided text. Could you summarize the provided text?", cat(content(myCorpus[[1]]), sep = "\n"))))
library(chatgpt)
library(openai)
library(httr)
library(httr)
library(httr)
knitr::opts_chunk$set(echo = TRUE)
# Define the URLs of the text documents
urls <- c(
"https://www.gutenberg.org/cache/epub/84/pg84.txt",
"https://www.gutenberg.org/cache/epub/2701/pg2701.txt",
"https://www.gutenberg.org/cache/epub/46/pg46.txt",
"https://www.gutenberg.org/cache/epub/1342/pg1342.txt",
"https://www.gutenberg.org/cache/epub/1513/pg1513.txt",
"https://www.gutenberg.org/cache/epub/145/pg145.txt",
"https://www.gutenberg.org/cache/epub/2641/pg2641.txt",
"https://www.gutenberg.org/cache/epub/100/pg100.txt",
"https://www.gutenberg.org/cache/epub/37106/pg37106.txt",
"https://www.gutenberg.org/cache/epub/16389/pg16389.txt",
"https://www.gutenberg.org/cache/epub/67979/pg67979.txt",
"https://www.gutenberg.org/cache/epub/394/pg394.txt",
"https://www.gutenberg.org/cache/epub/6761/pg6761.txt",
"https://www.gutenberg.org/cache/epub/2160/pg2160.txt",
"https://www.gutenberg.org/cache/epub/4085/pg4085.txt",
"https://www.gutenberg.org/cache/epub/6593/pg6593.txt",
"https://www.gutenberg.org/cache/epub/5197/pg5197.txt",
"https://www.gutenberg.org/cache/epub/1259/pg1259.txt",
"https://www.gutenberg.org/cache/epub/31516/pg31516.txt",
"https://www.gutenberg.org/cache/epub/11/pg11.txt"
)
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
library(wordcloud)
library(text)
library(tm)
library(SnowballC)
library(words)
library(NbClust)
library(stringr)
library(dplyr)
library(syuzhet)
# Define the URLs of the text documents
urls <- c(
"https://www.gutenberg.org/cache/epub/84/pg84.txt",
"https://www.gutenberg.org/cache/epub/2701/pg2701.txt",
"https://www.gutenberg.org/cache/epub/46/pg46.txt",
"https://www.gutenberg.org/cache/epub/1342/pg1342.txt",
"https://www.gutenberg.org/cache/epub/1513/pg1513.txt",
"https://www.gutenberg.org/cache/epub/145/pg145.txt",
"https://www.gutenberg.org/cache/epub/2641/pg2641.txt",
"https://www.gutenberg.org/cache/epub/100/pg100.txt",
"https://www.gutenberg.org/cache/epub/37106/pg37106.txt",
"https://www.gutenberg.org/cache/epub/16389/pg16389.txt",
"https://www.gutenberg.org/cache/epub/67979/pg67979.txt",
"https://www.gutenberg.org/cache/epub/394/pg394.txt",
"https://www.gutenberg.org/cache/epub/6761/pg6761.txt",
"https://www.gutenberg.org/cache/epub/2160/pg2160.txt",
"https://www.gutenberg.org/cache/epub/4085/pg4085.txt",
"https://www.gutenberg.org/cache/epub/6593/pg6593.txt",
"https://www.gutenberg.org/cache/epub/5197/pg5197.txt",
"https://www.gutenberg.org/cache/epub/1259/pg1259.txt",
"https://www.gutenberg.org/cache/epub/31516/pg31516.txt",
"https://www.gutenberg.org/cache/epub/11/pg11.txt"
)
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
library(httr)
library(chatgpt)
library(openai)
my_API <- "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"
hey_chatGPT <- function(answer_my_question) {
chat_GPT_answer <- POST(
url = "https://api.openai.com/v1/chat/completions",
add_headers(Authorization = paste("Bearer", my_API)),
content_type_json(),
encode = "json",
body = list(
model = "gpt-3.5-turbo-0301",
messages = list(
list(
role = "user",
content = answer_my_question
)
)
)
)
paste(str_trim(httr::content(chat_GPT_answer)$choices[[1]]$message$content), "TOKENS USED: ", httr::content(chat_GPT_answer)$usage$total_tokens)
}
paste(hey_chatGPT(paste("Look at the provided text. Could you summarize the provided text?", cat(content(myCorpus[[1]]), sep = "\n"))))
paste(hey_chatGPT(paste("Look at the provided text. Could you summarize the provided text?", cat(content(myCorpus[[1]]), sep = "\n"))))
content(myCorpus[[1]]
ï¼‰
content(myCorpus[[1]])
cat(content(myCorpus[[1]])
}
cat(content(myCorpus[[1]]))
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
# Define the URLs of the text documents
urls <- c(
"https://www.gutenberg.org/cache/epub/84/pg84.txt",
"https://www.gutenberg.org/cache/epub/2701/pg2701.txt",
"https://www.gutenberg.org/cache/epub/46/pg46.txt",
"https://www.gutenberg.org/cache/epub/1342/pg1342.txt",
"https://www.gutenberg.org/cache/epub/1513/pg1513.txt",
"https://www.gutenberg.org/cache/epub/145/pg145.txt",
"https://www.gutenberg.org/cache/epub/2641/pg2641.txt",
"https://www.gutenberg.org/cache/epub/100/pg100.txt",
"https://www.gutenberg.org/cache/epub/37106/pg37106.txt",
"https://www.gutenberg.org/cache/epub/16389/pg16389.txt",
"https://www.gutenberg.org/cache/epub/67979/pg67979.txt",
"https://www.gutenberg.org/cache/epub/394/pg394.txt",
"https://www.gutenberg.org/cache/epub/6761/pg6761.txt",
"https://www.gutenberg.org/cache/epub/2160/pg2160.txt",
"https://www.gutenberg.org/cache/epub/4085/pg4085.txt",
"https://www.gutenberg.org/cache/epub/6593/pg6593.txt",
"https://www.gutenberg.org/cache/epub/5197/pg5197.txt",
"https://www.gutenberg.org/cache/epub/1259/pg1259.txt",
"https://www.gutenberg.org/cache/epub/31516/pg31516.txt",
"https://www.gutenberg.org/cache/epub/11/pg11.txt"
)
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
knitr::opts_chunk$set(echo = TRUE)
library(wordcloud)
library(text)
library(tm)
library(SnowballC)
library(words)
library(NbClust)
library(stringr)
library(dplyr)
library(syuzhet)
# Define the URLs of the text documents
urls <- c(
"https://www.gutenberg.org/cache/epub/84/pg84.txt",
"https://www.gutenberg.org/cache/epub/2701/pg2701.txt",
"https://www.gutenberg.org/cache/epub/46/pg46.txt",
"https://www.gutenberg.org/cache/epub/1342/pg1342.txt",
"https://www.gutenberg.org/cache/epub/1513/pg1513.txt",
"https://www.gutenberg.org/cache/epub/145/pg145.txt",
"https://www.gutenberg.org/cache/epub/2641/pg2641.txt",
"https://www.gutenberg.org/cache/epub/100/pg100.txt",
"https://www.gutenberg.org/cache/epub/37106/pg37106.txt",
"https://www.gutenberg.org/cache/epub/16389/pg16389.txt",
"https://www.gutenberg.org/cache/epub/67979/pg67979.txt",
"https://www.gutenberg.org/cache/epub/394/pg394.txt",
"https://www.gutenberg.org/cache/epub/6761/pg6761.txt",
"https://www.gutenberg.org/cache/epub/2160/pg2160.txt",
"https://www.gutenberg.org/cache/epub/4085/pg4085.txt",
"https://www.gutenberg.org/cache/epub/6593/pg6593.txt",
"https://www.gutenberg.org/cache/epub/5197/pg5197.txt",
"https://www.gutenberg.org/cache/epub/1259/pg1259.txt",
"https://www.gutenberg.org/cache/epub/31516/pg31516.txt",
"https://www.gutenberg.org/cache/epub/11/pg11.txt"
)
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
library(httr)
library(chatgpt)
library(openai)
my_API <- "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"
hey_chatGPT <- function(answer_my_question) {
chat_GPT_answer <- POST(
url = "https://api.openai.com/v1/chat/completions",
add_headers(Authorization = paste("Bearer", my_API)),
content_type_json(),
encode = "json",
body = list(
model = "gpt-3.5-turbo-0301",
messages = list(
list(
role = "user",
content = answer_my_question
)
)
)
)
paste(str_trim(httr::content(chat_GPT_answer)$choices[[1]]$message$content), "TOKENS USED: ", httr::content(chat_GPT_answer)$usage$total_tokens)
}
paste(hey_chatGPT(paste("Look at the provided text. Could you summarize the provided text?", cat(content(myCorpus[[1]]), sep = "\n"))))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
knitr::opts_chunk$set(echo = TRUE)
library(wordcloud)
library(text)
library(tm)
library(SnowballC)
library(words)
library(NbClust)
library(stringr)
library(dplyr)
library(syuzhet)
# Define the URLs of the text documents
urls <- c(
"https://www.gutenberg.org/cache/epub/84/pg84.txt",
"https://www.gutenberg.org/cache/epub/2701/pg2701.txt",
"https://www.gutenberg.org/cache/epub/46/pg46.txt",
"https://www.gutenberg.org/cache/epub/1342/pg1342.txt",
"https://www.gutenberg.org/cache/epub/1513/pg1513.txt",
"https://www.gutenberg.org/cache/epub/145/pg145.txt",
"https://www.gutenberg.org/cache/epub/2641/pg2641.txt",
"https://www.gutenberg.org/cache/epub/100/pg100.txt",
"https://www.gutenberg.org/cache/epub/37106/pg37106.txt",
"https://www.gutenberg.org/cache/epub/16389/pg16389.txt",
"https://www.gutenberg.org/cache/epub/67979/pg67979.txt",
"https://www.gutenberg.org/cache/epub/394/pg394.txt",
"https://www.gutenberg.org/cache/epub/6761/pg6761.txt",
"https://www.gutenberg.org/cache/epub/2160/pg2160.txt",
"https://www.gutenberg.org/cache/epub/4085/pg4085.txt",
"https://www.gutenberg.org/cache/epub/6593/pg6593.txt",
"https://www.gutenberg.org/cache/epub/5197/pg5197.txt",
"https://www.gutenberg.org/cache/epub/1259/pg1259.txt",
"https://www.gutenberg.org/cache/epub/31516/pg31516.txt",
"https://www.gutenberg.org/cache/epub/11/pg11.txt"
)
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
cat(content(myCorpus[[1]]), sep = "\n")
text
myText = cat(content(myCorpus[[1]]), sep = "\n")
paste(hey_chatGPT(paste("Look at the provided text. Could you summarize the provided text?", myText)))
library(httr)
library(chatgpt)
library(openai)
my_API <- "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"
hey_chatGPT <- function(answer_my_question) {
chat_GPT_answer <- httr::POST(
url = "https://api.openai.com/v1/chat/completions",
add_headers(Authorization = paste("Bearer", my_API)),
content_type_json(),
encode = "json",
body = list(
model = "gpt-3.5-turbo-0301",
messages = list(
list(
role = "user",
content = answer_my_question
)
)
)
)
paste(str_trim(httr::content(chat_GPT_answer)$choices[[1]]$message$content), "TOKENS USED: ", httr::content(chat_GPT_answer)$usage$total_tokens)
}
paste(hey_chatGPT(paste("Look at the provided text. Could you summarize the provided text?", myText)))
myText$summarization <- NULL
myText$summarization[1] <- paste(hey_chatGPT(paste("Look at the provided text. Could you summarize the provided text?", myText)))
myText$summarization <- NULL
myText$summarization[1] <- paste(hey_chatGPT(paste("Look at the provided text. Could you summarize the provided text?", myText)))
head(myText)
myText
myText = cat(content(myCorpus[[1]]), sep = "\n")
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myText = cat(content(myCorpus[[1]]), sep = "\n")
knitr::opts_chunk$set(echo = TRUE)
library(wordcloud)
library(text)
library(tm)
library(SnowballC)
library(words)
library(NbClust)
library(stringr)
library(dplyr)
library(syuzhet)
# Define the URLs of the text documents
urls <- c(
"https://www.gutenberg.org/cache/epub/84/pg84.txt",
"https://www.gutenberg.org/cache/epub/2701/pg2701.txt",
"https://www.gutenberg.org/cache/epub/46/pg46.txt",
"https://www.gutenberg.org/cache/epub/1342/pg1342.txt",
"https://www.gutenberg.org/cache/epub/1513/pg1513.txt",
"https://www.gutenberg.org/cache/epub/145/pg145.txt",
"https://www.gutenberg.org/cache/epub/2641/pg2641.txt",
"https://www.gutenberg.org/cache/epub/100/pg100.txt",
"https://www.gutenberg.org/cache/epub/37106/pg37106.txt",
"https://www.gutenberg.org/cache/epub/16389/pg16389.txt",
"https://www.gutenberg.org/cache/epub/67979/pg67979.txt",
"https://www.gutenberg.org/cache/epub/394/pg394.txt",
"https://www.gutenberg.org/cache/epub/6761/pg6761.txt",
"https://www.gutenberg.org/cache/epub/2160/pg2160.txt",
"https://www.gutenberg.org/cache/epub/4085/pg4085.txt",
"https://www.gutenberg.org/cache/epub/6593/pg6593.txt",
"https://www.gutenberg.org/cache/epub/5197/pg5197.txt",
"https://www.gutenberg.org/cache/epub/1259/pg1259.txt",
"https://www.gutenberg.org/cache/epub/31516/pg31516.txt",
"https://www.gutenberg.org/cache/epub/11/pg11.txt"
)
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
library(httr)
library(chatgpt)
library(openai)
my_API <- "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"
summarize_text <- function(text) {
api_url <- "https://api.openai.com/v1/engines/gpt-3.5-turbo/completions" #
api_key <- my_API
body <- list(
prompt = paste("Summarize this text:", text),
max_tokens = 100
)
headers <- c(
'Content-Type' = 'application/json',
'Authorization' = paste('Bearer', api_key)
)
response <- POST(api_url, body = toJSON(body), add_headers(headers))
result <- content(response, "parsed")
return(result$choices[[1]]$text)
}
summaries <- lapply(myCorpus, function(doc) {
text <- as.character(doc)
summarize_text(text)
})
library(jsonlite)
library(httr)
library(chatgpt)
library(openai)
library(jsonlite)
my_API <- "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"
summarize_text <- function(text) {
api_url <- "https://api.openai.com/v1/engines/gpt-3.5-turbo/completions" #
api_key <- my_API
body <- list(
prompt = paste("Summarize this text:", text),
max_tokens = 100
)
headers <- c(
'Content-Type' = 'application/json',
'Authorization' = paste('Bearer', api_key)
)
response <- POST(api_url, body = toJSON(body), add_headers(headers))
result <- content(response, "parsed")
return(result$choices[[1]]$text)
}
summaries <- lapply(myCorpus, function(doc) {
text <- as.character(doc)
summarize_text(text)
})
summaries
print(myCorpus[[1]])
test_summary <- summarize_text("This is a test text.")
print(test_summary)
library(httr)
library(chatgpt)
library(openai)
library(jsonlite)
my_API <- "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"
summarize_text <- function(text) {
api_url <- "https://api.openai.com/v1/engines/gpt-3.5-turbo/completions" #
api_key <- my_API
body <- list(
prompt = paste("Summarize this text:", text),
max_tokens = 100
)
headers <- c(
'Content-Type' = 'application/json',
'Authorization' = paste('Bearer', api_key)
)
response <- POST(api_url, body = toJSON(body), add_headers(headers))
result <- content(response, "parsed")
return(result$choices[[1]]$text)
}
summaries <- lapply(myCorpus[[1]], function(doc) {
text <- as.character(doc)
summarize_text(text)
})
summaries
myCorpus[[1]]
(content(myCorpus[[1]])
)[0:5]
myCorpus[[1]]
cat(content(myCorpus[[1]])[830:900], sep = "\n")
knitr::opts_chunk$set(echo = TRUE)
library(wordcloud)
library(text)
library(tm)
library(SnowballC)
library(words)
library(NbClust)
library(stringr)
library(dplyr)
library(syuzhet)
# Define the URLs of the text documents
urls <- c(
"https://www.gutenberg.org/cache/epub/84/pg84.txt",
"https://www.gutenberg.org/cache/epub/2701/pg2701.txt",
"https://www.gutenberg.org/cache/epub/46/pg46.txt",
"https://www.gutenberg.org/cache/epub/1342/pg1342.txt",
"https://www.gutenberg.org/cache/epub/1513/pg1513.txt",
"https://www.gutenberg.org/cache/epub/145/pg145.txt",
"https://www.gutenberg.org/cache/epub/2641/pg2641.txt",
"https://www.gutenberg.org/cache/epub/100/pg100.txt",
"https://www.gutenberg.org/cache/epub/37106/pg37106.txt",
"https://www.gutenberg.org/cache/epub/16389/pg16389.txt",
"https://www.gutenberg.org/cache/epub/67979/pg67979.txt",
"https://www.gutenberg.org/cache/epub/394/pg394.txt",
"https://www.gutenberg.org/cache/epub/6761/pg6761.txt",
"https://www.gutenberg.org/cache/epub/2160/pg2160.txt",
"https://www.gutenberg.org/cache/epub/4085/pg4085.txt",
"https://www.gutenberg.org/cache/epub/6593/pg6593.txt",
"https://www.gutenberg.org/cache/epub/5197/pg5197.txt",
"https://www.gutenberg.org/cache/epub/1259/pg1259.txt",
"https://www.gutenberg.org/cache/epub/31516/pg31516.txt",
"https://www.gutenberg.org/cache/epub/11/pg11.txt"
)
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
content(myCorpus[[1]])[830:900]
myCorpus[[1]]
library(httr)
library(chatgpt)
library(openai)
library(jsonlite)
my_API <- "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"
summarize_text <- function(text) {
api_url <- "https://api.openai.com/v1/engines/gpt-3.5-turbo/completions" #
api_key <- my_API
body <- list(
prompt = paste("Summarize this text:", text),
max_tokens = 100
)
headers <- c(
'Content-Type' = 'application/json',
'Authorization' = paste('Bearer', api_key)
)
response <- POST(api_url, body = toJSON(body), add_headers(headers))
result <- content(response, "parsed")
return(result$choices[[1]]$text)
}
summaries <- lapply(myCorpus[[1]], function(doc) {
text <- as.character(doc)
summarize_text(text)
})
summaries
myCorpus[[1]]
cat(content(myCorpus[[1]])[830:900], sep = "\n")
myCorpus[[1]]
myCorpus <- tm::tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removePunctuation)
cat(content(myCorpus[[1]])[830:900], sep = "\n")
