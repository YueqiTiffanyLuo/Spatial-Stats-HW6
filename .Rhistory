knitr::opts_chunk$set(echo = TRUE)
library(wordcloud)
library(text)
library(tm)
library(SnowballC)
library(words)
library(NbClust)
library(stringr)
library(dplyr)
library(syuzhet)
# Define the URLs of the text documents
urls <- c(
"https://www.gutenberg.org/cache/epub/84/pg84.txt",
"https://www.gutenberg.org/cache/epub/2701/pg2701.txt",
"https://www.gutenberg.org/cache/epub/46/pg46.txt",
"https://www.gutenberg.org/cache/epub/1342/pg1342.txt",
"https://www.gutenberg.org/cache/epub/1513/pg1513.txt",
"https://www.gutenberg.org/cache/epub/145/pg145.txt",
"https://www.gutenberg.org/cache/epub/2641/pg2641.txt",
"https://www.gutenberg.org/cache/epub/100/pg100.txt",
"https://www.gutenberg.org/cache/epub/37106/pg37106.txt",
"https://www.gutenberg.org/cache/epub/16389/pg16389.txt",
"https://www.gutenberg.org/cache/epub/67979/pg67979.txt",
"https://www.gutenberg.org/cache/epub/394/pg394.txt",
"https://www.gutenberg.org/cache/epub/6761/pg6761.txt",
"https://www.gutenberg.org/cache/epub/2160/pg2160.txt",
"https://www.gutenberg.org/cache/epub/4085/pg4085.txt",
"https://www.gutenberg.org/cache/epub/6593/pg6593.txt",
"https://www.gutenberg.org/cache/epub/5197/pg5197.txt",
"https://www.gutenberg.org/cache/epub/1259/pg1259.txt",
"https://www.gutenberg.org/cache/epub/31516/pg31516.txt",
"https://www.gutenberg.org/cache/epub/11/pg11.txt"
)
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
knitr::opts_chunk$set(echo = TRUE)
library(wordcloud)
library(text)
library(tm)
library(SnowballC)
library(words)
library(NbClust)
library(stringr)
library(dplyr)
library(syuzhet)
# Define the URLs of the text documents
urls <- c(
"https://www.gutenberg.org/cache/epub/84/pg84.txt",
"https://www.gutenberg.org/cache/epub/2701/pg2701.txt",
"https://www.gutenberg.org/cache/epub/46/pg46.txt",
"https://www.gutenberg.org/cache/epub/1342/pg1342.txt",
"https://www.gutenberg.org/cache/epub/1513/pg1513.txt",
"https://www.gutenberg.org/cache/epub/145/pg145.txt",
"https://www.gutenberg.org/cache/epub/2641/pg2641.txt",
"https://www.gutenberg.org/cache/epub/100/pg100.txt",
"https://www.gutenberg.org/cache/epub/37106/pg37106.txt",
"https://www.gutenberg.org/cache/epub/16389/pg16389.txt",
"https://www.gutenberg.org/cache/epub/67979/pg67979.txt",
"https://www.gutenberg.org/cache/epub/394/pg394.txt",
"https://www.gutenberg.org/cache/epub/6761/pg6761.txt",
"https://www.gutenberg.org/cache/epub/2160/pg2160.txt",
"https://www.gutenberg.org/cache/epub/4085/pg4085.txt",
"https://www.gutenberg.org/cache/epub/6593/pg6593.txt",
"https://www.gutenberg.org/cache/epub/5197/pg5197.txt",
"https://www.gutenberg.org/cache/epub/1259/pg1259.txt",
"https://www.gutenberg.org/cache/epub/31516/pg31516.txt",
"https://www.gutenberg.org/cache/epub/11/pg11.txt"
)
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
#     Defining the toSpace function
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#     Defining the remApostrophe function
remApostrophe <- content_transformer(function(x,pattern) gsub(pattern, "", x))
#     Removing special characters
myCorpus <- tm_map(myCorpus, toSpace, "@")
myCorpus <- tm_map(myCorpus, toSpace, "/")
myCorpus <- tm_map(myCorpus, toSpace, "]")
myCorpus <- tm_map(myCorpus, toSpace, "$")
myCorpus <- tm_map(myCorpus, toSpace, "—")
myCorpus <- tm_map(myCorpus, toSpace, "‐")
myCorpus <- tm_map(myCorpus, toSpace, "”")
myCorpus <- tm_map(myCorpus, toSpace, "‘")
myCorpus <- tm_map(myCorpus, toSpace, "“")
myCorpus <- tm_map(myCorpus, toSpace, "‘")
myCorpus <- tm_map(myCorpus, remApostrophe, "’")
myCorpus <- tm::tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removePunctuation)
cat(content(myCorpus[[1]])[830:900], sep = "\n")
stopwords("english")
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
myCorpus <- tm_map(myCorpus, removeWords,c("ebook", "author", "translators",
"start wwwgutenbergorg", "gutenberg", "chapter", "book", "contents"))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
myCorpus <- tm_map(myCorpus, stemDocument)
cat(content(myCorpus[[1]])[830:900], sep = "\n")
tdm <- TermDocumentMatrix(myCorpus)
tm::inspect(tdm)
m<- as.matrix(tdm)
dim(m)
rownames(m) <- tdm$dimnames$Terms
colnames(m) <- c("Frankenstein", "Moby Dick", "A Christmas Carol in Prose", "Pride and Prejudice", "Romeo and Juliet", "Middlemarch", "A Room with a View",
"The Complete Works of William Shakespeare", "Little Women", "The Enchanted April", "The Blue Castle", "Cranford", "The Adventures of Ferdinand Count Fathom", "The Expedition of Humphry Clinker", "The Adventures of Roderick Random", "History of Tom Jones, a Foundling", "My Life — Volume 1", "Twenty years after", "The Eyes Have It", "Alice's Adventures in Wonderland")
head(m)
dictionary <- as.character(words::words$word)
row_names <- rownames(m)
in_dictionary <- row_names %in% dictionary
remove <- as.character(row_names[!in_dictionary])
#Since the data are so large, if we try to remove all words at once, we get an error. So we will remove them in chunks of 1000.
num_observations <- as.numeric(length(remove))  # Total number of observations
chunk_size <- 1000                              # Number of observations to display at a time
for (i in seq(1, num_observations, chunk_size)) {
start <- i
end <- i + chunk_size - 1
end <- ifelse(end > num_observations, num_observations, end)
myCorpus <- tm_map(myCorpus, removeWords, remove[start:end])
}
dictionary <- as.character(words::words$word)
row_names <- rownames(m)
in_dictionary <- row_names %in% dictionary
remove <- as.character(row_names[!in_dictionary])
#Since the data are so large, if we try to remove all words at once, we get an error. So we will remove them in chunks of 1000.
num_observations <- as.numeric(length(remove))  # Total number of observations
chunk_size <- 1000                              # Number of observations to display at a time
for (i in seq(1, num_observations, chunk_size)) {
start <- i
end <- i + chunk_size - 1
end <- ifelse(end > num_observations, num_observations, end)
myCorpus <- tm_map(myCorpus, removeWords, remove[start:end])
}
cat(content(myCorpus[[1]])[980:1000], sep = "\n")
dtm_cleaned <- DocumentTermMatrix(myCorpus)
tm::inspect(dtm_cleaned)
m <- as.matrix(dtm_cleaned)
dim(m)
colnames(m) <- dtm_cleaned$dimnames$Terms
rownames(m) <- c("Frankenstein", "Moby Dick", "A Christmas Carol in Prose", "Pride and Prejudice", "Romeo and Juliet", "Middlemarch", "A Room with a View",
"The Complete Works of William Shakespeare", "Little Women", "The Enchanted April", "The Blue Castle", "Cranford", "The Adventures of Ferdinand Count Fathom", "The Expedition of Humphry Clinker", "The Adventures of Roderick Random", "History of Tom Jones, a Foundling", "My Life — Volume 1", "Twenty years after", "The Eyes Have It", "Alice's Adventures in Wonderland")
cs <- as.matrix(colSums(m))             #How many times each term appears across all documents (texts)
rownames(cs) <- dtm_cleaned$dimnames$Terms
hist(cs, breaks=100)                    #histograms/tabulations/word cloud of total term appearance
tab <- as.matrix(table(cs))
wordcloud(myCorpus, min.freq=1000)
variables_to_remove <- cs < 10000
# Subset matrix frame, excluding those variables
m_subset <- m[, !variables_to_remove]
#Some books are longer, others are shorter. We divide the frequencies by the total number of words (after processing) in each book.
m_fin <- m_subset/rowSums(m)
#Let's scale (normalize) each of the variables (relative frequency)
m_scale <- scale(m_fin)
#NbClust approach
set.seed(1234)
nc <- NbClust(m_scale, min.nc=2, max.nc=15, method="kmeans", index="all")
#table(nc$Best.n[1,])
par(mfrow=c(1,1))
barplot(table(nc$Best.n[1,]),
xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
k_means_results <- kmeans(m_scale, 3, 30)
k_means_results$cluster
k_means_results$size
word_totals_by_cluster <- round(aggregate(m_subset, by=list(cluster=k_means_results$cluster), sum),1)
#plot the results
#Decrease font size and rotate x-axis labels vertically
par(cex.axis = 0.7)  # Adjust the font size
par(las = 2)        # Rotate labels vertically
barplot(as.matrix(word_totals_by_cluster[-1]),
beside = TRUE,
col = c("blue", "green"),
legend.text = TRUE,
args.legend = list(x = "topright"))
# Add labels to the x-axis and y-axis. Here, the first cluster is in blue and the second one is in green.
title(xlab = "Cluster")
title(ylab = "Sum")
# Add a title to the plot
title(main = "Bar Plot of Sums by Group")
nrc <- syuzhet::get_sentiment_dictionary(dictionary="nrc")
head(nrc, n=20L)
afinn <- syuzhet::get_sentiment_dictionary(dictionary="afinn")
head(afinn, n=20L)
bing <- syuzhet::get_sentiment_dictionary(dictionary="bing")
head(bing, n=20L)
syuzhet <- syuzhet::get_sentiment_dictionary(dictionary="syuzhet")
head(syuzhet, n=20L)
get_nrc_sentiment("gorgeous")
Frankenstein <- as.data.frame(m[1,])
Frankenstein$Term <- as.vector(rownames(Frankenstein))
colnames(Frankenstein)[1] = "Term_Frequency"
rownames(Frankenstein) <- 1:nrow(Frankenstein)
nrc_sentiment <- get_nrc_sentiment(Frankenstein$Term)
Frankenstein_Sentiment <- cbind(Frankenstein, nrc_sentiment)
head(Frankenstein)
# Select the columns to be multiplied (last ten columns)
cols_to_multiply <- names(Frankenstein_Sentiment)[3:12]
# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Frankenstein_Sentiment[, cols_to_multiply] <- Frankenstein_Sentiment[, cols_to_multiply] * Frankenstein_Sentiment$Term_Frequency
Frankenstein_Sentiment_Total <- t(as.matrix(colSums(Frankenstein_Sentiment[,-1:-2])))
barplot(Frankenstein_Sentiment_Total, las=2, ylab='Count', main='Sentiment Scores')
Frankenstein$Syuzhet <- as.matrix(get_sentiment(Frankenstein$Term, method="syuzhet"))
hist(Frankenstein$Syuzhet)
Frankenstein$Bing <- as.matrix(get_sentiment(Frankenstein$Term, method="bing"))
hist(Frankenstein$Bing)
Frankenstein$AFINN <- as.matrix(get_sentiment(Frankenstein$Term, method="afinn"))
hist(Frankenstein$AFINN)
Frankenstein$NRC <- as.matrix(get_sentiment(Frankenstein$Term, method="nrc"))   #There are Negative and Positive sentiments in the NRC output above.
hist(Frankenstein$NRC)
sentiment_columns <- Frankenstein[ , 3:6]
sentiment_columns <- data.frame(lapply(sentiment_columns, sign))
sentiment_columns <- data.frame(lapply(sentiment_columns, as.factor))
#Raw frequencies
sapply(sentiment_columns, function(x) if("factor" %in% class(x)) {table(x)})
#Proportions
sapply(sentiment_columns, function(x) if("factor" %in% class(x)) {prop.table(table(x))})
knitr::opts_chunk$set(echo = TRUE)
#NbClust approach
set.seed(1234)
nc <- NbClust(m_scale, min.nc=2, max.nc=15, method="kmeans", index="all")
knitr::opts_chunk$set(echo = TRUE)
library(wordcloud)
library(text)
library(tm)
library(SnowballC)
library(words)
library(NbClust)
library(stringr)
library(dplyr)
library(syuzhet)
# Define the URLs of the text documents
urls <- c(
"https://www.gutenberg.org/cache/epub/84/pg84.txt",
"https://www.gutenberg.org/cache/epub/2701/pg2701.txt",
"https://www.gutenberg.org/cache/epub/46/pg46.txt",
"https://www.gutenberg.org/cache/epub/1342/pg1342.txt",
"https://www.gutenberg.org/cache/epub/1513/pg1513.txt",
"https://www.gutenberg.org/cache/epub/145/pg145.txt",
"https://www.gutenberg.org/cache/epub/2641/pg2641.txt",
"https://www.gutenberg.org/cache/epub/100/pg100.txt",
"https://www.gutenberg.org/cache/epub/37106/pg37106.txt",
"https://www.gutenberg.org/cache/epub/16389/pg16389.txt",
"https://www.gutenberg.org/cache/epub/67979/pg67979.txt",
"https://www.gutenberg.org/cache/epub/394/pg394.txt",
"https://www.gutenberg.org/cache/epub/6761/pg6761.txt",
"https://www.gutenberg.org/cache/epub/2160/pg2160.txt",
"https://www.gutenberg.org/cache/epub/4085/pg4085.txt",
"https://www.gutenberg.org/cache/epub/6593/pg6593.txt",
"https://www.gutenberg.org/cache/epub/5197/pg5197.txt",
"https://www.gutenberg.org/cache/epub/1259/pg1259.txt",
"https://www.gutenberg.org/cache/epub/31516/pg31516.txt",
"https://www.gutenberg.org/cache/epub/11/pg11.txt"
)
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
#     Defining the toSpace function
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#     Defining the remApostrophe function
remApostrophe <- content_transformer(function(x,pattern) gsub(pattern, "", x))
#     Removing special characters
myCorpus <- tm_map(myCorpus, toSpace, "@")
myCorpus <- tm_map(myCorpus, toSpace, "/")
myCorpus <- tm_map(myCorpus, toSpace, "]")
myCorpus <- tm_map(myCorpus, toSpace, "$")
myCorpus <- tm_map(myCorpus, toSpace, "—")
myCorpus <- tm_map(myCorpus, toSpace, "‐")
myCorpus <- tm_map(myCorpus, toSpace, "”")
myCorpus <- tm_map(myCorpus, toSpace, "‘")
myCorpus <- tm_map(myCorpus, toSpace, "“")
myCorpus <- tm_map(myCorpus, toSpace, "‘")
myCorpus <- tm_map(myCorpus, remApostrophe, "’")
myCorpus <- tm::tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removePunctuation)
cat(content(myCorpus[[1]])[830:900], sep = "\n")
stopwords("english")
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
myCorpus <- tm_map(myCorpus, removeWords,c("ebook", "author", "translators",
"start wwwgutenbergorg", "gutenberg", "chapter", "book", "contents"))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
tdm <- TermDocumentMatrix(myCorpus)
tm::inspect(tdm)
m<- as.matrix(tdm)
dim(m)
rownames(m) <- tdm$dimnames$Terms
colnames(m) <- c("Frankenstein", "Moby Dick", "A Christmas Carol in Prose", "Pride and Prejudice", "Romeo and Juliet", "Middlemarch", "A Room with a View",
"The Complete Works of William Shakespeare", "Little Women", "The Enchanted April", "The Blue Castle", "Cranford", "The Adventures of Ferdinand Count Fathom", "The Expedition of Humphry Clinker", "The Adventures of Roderick Random", "History of Tom Jones, a Foundling", "My Life — Volume 1", "Twenty years after", "The Eyes Have It", "Alice's Adventures in Wonderland")
head(m)
dictionary <- as.character(words::words$word)
row_names <- rownames(m)
in_dictionary <- row_names %in% dictionary
remove <- as.character(row_names[!in_dictionary])
#Since the data are so large, if we try to remove all words at once, we get an error. So we will remove them in chunks of 1000.
num_observations <- as.numeric(length(remove))  # Total number of observations
chunk_size <- 1000                              # Number of observations to display at a time
for (i in seq(1, num_observations, chunk_size)) {
start <- i
end <- i + chunk_size - 1
end <- ifelse(end > num_observations, num_observations, end)
myCorpus <- tm_map(myCorpus, removeWords, remove[start:end])
}
cat(content(myCorpus[[1]])[980:1000], sep = "\n")
dtm_cleaned <- DocumentTermMatrix(myCorpus)
tm::inspect(dtm_cleaned)
m <- as.matrix(dtm_cleaned)
dim(m)
colnames(m) <- dtm_cleaned$dimnames$Terms
rownames(m) <- c("Frankenstein", "Moby Dick", "A Christmas Carol in Prose", "Pride and Prejudice", "Romeo and Juliet", "Middlemarch", "A Room with a View",
"The Complete Works of William Shakespeare", "Little Women", "The Enchanted April", "The Blue Castle", "Cranford", "The Adventures of Ferdinand Count Fathom", "The Expedition of Humphry Clinker", "The Adventures of Roderick Random", "History of Tom Jones, a Foundling", "My Life — Volume 1", "Twenty years after", "The Eyes Have It", "Alice's Adventures in Wonderland")
cs <- as.matrix(colSums(m))             #How many times each term appears across all documents (texts)
rownames(cs) <- dtm_cleaned$dimnames$Terms
hist(cs, breaks=100)                    #histograms/tabulations/word cloud of total term appearance
tab <- as.matrix(table(cs))
wordcloud(myCorpus, min.freq=1000)
variables_to_remove <- cs < 10000
# Subset matrix frame, excluding those variables
m_subset <- m[, !variables_to_remove]
#Some books are longer, others are shorter. We divide the frequencies by the total number of words (after processing) in each book.
m_fin <- m_subset/rowSums(m)
#Let's scale (normalize) each of the variables (relative frequency)
m_scale <- scale(m_fin)
#NbClust approach
set.seed(1234)
nc <- NbClust(m_scale, min.nc=2, max.nc=15, method="kmeans", index="all")
#table(nc$Best.n[1,])
par(mfrow=c(1,1))
barplot(table(nc$Best.n[1,]),
xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
k_means_results <- kmeans(m_scale, 3, 30)
k_means_results$cluster
k_means_results$size
word_totals_by_cluster <- round(aggregate(m_subset, by=list(cluster=k_means_results$cluster), sum),1)
#plot the results
#Decrease font size and rotate x-axis labels vertically
par(cex.axis = 0.7)  # Adjust the font size
par(las = 2)        # Rotate labels vertically
barplot(as.matrix(word_totals_by_cluster[-1]),
beside = TRUE,
col = c("blue", "green"),
legend.text = TRUE,
args.legend = list(x = "topright"))
# Add labels to the x-axis and y-axis. Here, the first cluster is in blue and the second one is in green.
title(xlab = "Cluster")
title(ylab = "Sum")
# Add a title to the plot
title(main = "Bar Plot of Sums by Group")
nrc <- syuzhet::get_sentiment_dictionary(dictionary="nrc")
head(nrc, n=20L)
afinn <- syuzhet::get_sentiment_dictionary(dictionary="afinn")
head(afinn, n=20L)
bing <- syuzhet::get_sentiment_dictionary(dictionary="bing")
head(bing, n=20L)
syuzhet <- syuzhet::get_sentiment_dictionary(dictionary="syuzhet")
head(syuzhet, n=20L)
get_nrc_sentiment("gorgeous")
Frankenstein <- as.data.frame(m[1,])
Frankenstein$Term <- as.vector(rownames(Frankenstein))
colnames(Frankenstein)[1] = "Term_Frequency"
rownames(Frankenstein) <- 1:nrow(Frankenstein)
nrc_sentiment <- get_nrc_sentiment(Frankenstein$Term)
Frankenstein_Sentiment <- cbind(Frankenstein, nrc_sentiment)
head(Frankenstein)
# Select the columns to be multiplied (last ten columns)
cols_to_multiply <- names(Frankenstein_Sentiment)[3:12]
# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Frankenstein_Sentiment[, cols_to_multiply] <- Frankenstein_Sentiment[, cols_to_multiply] * Frankenstein_Sentiment$Term_Frequency
Frankenstein_Sentiment_Total <- t(as.matrix(colSums(Frankenstein_Sentiment[,-1:-2])))
barplot(Frankenstein_Sentiment_Total, las=2, ylab='Count', main='Sentiment Scores')
Frankenstein$Syuzhet <- as.matrix(get_sentiment(Frankenstein$Term, method="syuzhet"))
hist(Frankenstein$Syuzhet)
Frankenstein$Bing <- as.matrix(get_sentiment(Frankenstein$Term, method="bing"))
hist(Frankenstein$Bing)
Frankenstein$AFINN <- as.matrix(get_sentiment(Frankenstein$Term, method="afinn"))
hist(Frankenstein$AFINN)
Frankenstein$NRC <- as.matrix(get_sentiment(Frankenstein$Term, method="nrc"))   #There are Negative and Positive sentiments in the NRC output above.
hist(Frankenstein$NRC)
sentiment_columns <- Frankenstein[ , 3:6]
sentiment_columns <- data.frame(lapply(sentiment_columns, sign))
sentiment_columns <- data.frame(lapply(sentiment_columns, as.factor))
#Raw frequencies
sapply(sentiment_columns, function(x) if("factor" %in% class(x)) {table(x)})
#Proportions
sapply(sentiment_columns, function(x) if("factor" %in% class(x)) {prop.table(table(x))})
library(wordcloud)
library(text)
library(tm)
library(SnowballC)
library(words)
library(NbClust)
library(stringr)
library(dplyr)
library(syuzhet)
# Define the URLs of the text documents
urls <- c(
"https://www.gutenberg.org/cache/epub/84/pg84.txt",
"https://www.gutenberg.org/cache/epub/2701/pg2701.txt",
"https://www.gutenberg.org/cache/epub/46/pg46.txt",
"https://www.gutenberg.org/cache/epub/1342/pg1342.txt",
"https://www.gutenberg.org/cache/epub/1513/pg1513.txt",
"https://www.gutenberg.org/cache/epub/145/pg145.txt",
"https://www.gutenberg.org/cache/epub/2641/pg2641.txt",
"https://www.gutenberg.org/cache/epub/100/pg100.txt",
"https://www.gutenberg.org/cache/epub/37106/pg37106.txt",
"https://www.gutenberg.org/cache/epub/16389/pg16389.txt",
"https://www.gutenberg.org/cache/epub/67979/pg67979.txt",
"https://www.gutenberg.org/cache/epub/394/pg394.txt",
"https://www.gutenberg.org/cache/epub/6761/pg6761.txt",
"https://www.gutenberg.org/cache/epub/2160/pg2160.txt",
"https://www.gutenberg.org/cache/epub/4085/pg4085.txt",
"https://www.gutenberg.org/cache/epub/6593/pg6593.txt",
"https://www.gutenberg.org/cache/epub/5197/pg5197.txt",
"https://www.gutenberg.org/cache/epub/1259/pg1259.txt",
"https://www.gutenberg.org/cache/epub/31516/pg31516.txt",
"https://www.gutenberg.org/cache/epub/11/pg11.txt"
)
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(sapply(urls, readLines)))
# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
cat(content(myCorpus[[1]])[830:900], sep = "\n")
